{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_repositories(github_url):\n",
    "    # Extract the username from the GitHub URL\n",
    "    username = github_url.split(\"/\")[-1]\n",
    "\n",
    "    # Make the API request to retrieve the user's repositories\n",
    "    url = f\"https://api.github.com/users/{username}/repos\"\n",
    "    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response and extract the repository names and URLs\n",
    "        repositories = []\n",
    "        data = response.json()\n",
    "        for repo in data:\n",
    "            repo_name = repo[\"name\"]\n",
    "            repo_url = repo[\"html_url\"]\n",
    "            repositories.append({\"name\": repo_name, \"url\": repo_url})\n",
    "\n",
    "        return repositories\n",
    "    else:\n",
    "        # Handle API request errors\n",
    "        print(\"Error: Failed to fetch user repositories.\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_code(repository):\n",
    "    processed_files = set()\n",
    "    contents = preprocess_files(repository)\n",
    "    preprocessed_contents = []\n",
    "    for file in contents:\n",
    "        file_type = file[\"type\"]\n",
    "        content = file[\"content\"]\n",
    "        if file_type == \"jupyter_notebook\":\n",
    "            preprocessed_contents.append(preprocess_jupyter_notebook(content))\n",
    "        elif file_type == \"package_file\":\n",
    "            preprocessed_contents.append(preprocess_package_file(content))\n",
    "        elif file_type == \"regular_file\":\n",
    "            preprocessed_contents.append(preprocess_regular_file(content))\n",
    "\n",
    "    return preprocessed_contents\n",
    "\n",
    "def preprocess_files(repository):\n",
    "    files = fetch_repository_files(repository)\n",
    "    contents = []\n",
    "    for file in files:\n",
    "        file_path = file[\"name\"]\n",
    "        content = fetch_file_content(file[\"download_url\"])\n",
    "        contents.append({\"name\": file_path, \"type\": file[\"type\"], \"content\": content})\n",
    "\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_repository_files(repository):\n",
    "    url = f\"https://api.github.com/repos/suhasml/{repository['name']}/contents\"\n",
    "    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        files = []\n",
    "        data = response.json()\n",
    "        fetch_files_recursive(data, files)\n",
    "\n",
    "        return files\n",
    "    else:\n",
    "        print(f\"Error: Failed to fetch files in repository {repository['name']}.\")\n",
    "        return []\n",
    "\n",
    "def fetch_files_recursive(data, files):\n",
    "    for item in data:\n",
    "        if item[\"type\"] == \"file\":\n",
    "            file_name = item[\"name\"]\n",
    "            file_extension = file_name.split(\".\")[-1].lower()\n",
    "            if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"ico\", \"h5\", \"pkl\", \"gitignore\", \"json\", \"node\"]:\n",
    "                file_type = determine_file_type(file_name)\n",
    "                files.append({\"name\": file_name, \"type\": file_type, \"download_url\": item[\"download_url\"]})\n",
    "        elif item[\"type\"] == \"dir\":\n",
    "            url = item[\"url\"]\n",
    "            headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                subdir_data = response.json()\n",
    "                fetch_files_recursive(subdir_data, files)\n",
    "            else:\n",
    "                print(f\"Error: Failed to fetch files in directory {item['name']}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_file_type(file_name):\n",
    "    if file_name.endswith(\".ipynb\"):\n",
    "        return \"jupyter_notebook\"\n",
    "    elif file_name.endswith(\".py\"):\n",
    "        return \"package_file\"\n",
    "    elif file_name.endswith(\".h5\") or file_name.endswith(\".pkl\"):\n",
    "        return \"binary_file\"\n",
    "    else:\n",
    "        return \"regular_file\"\n",
    "\n",
    "def fetch_file_content(download_url):\n",
    "    response = requests.get(download_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        print(f\"Error: Failed to fetch file content from {download_url}.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_jupyter_notebook(content):\n",
    "    notebook = nbformat.reads(content, nbformat.NO_CONVERT)\n",
    "    preprocessed_cells = []\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == \"code\":\n",
    "            preprocessed_cells.append(preprocess_code_cell(cell))\n",
    "\n",
    "    return preprocessed_cells\n",
    "\n",
    "def preprocess_package_file(content):\n",
    "    # Implement your preprocessing logic for package files\n",
    "    # You can limit the token count or chunk the file as necessary\n",
    "    # Example: Limit the token count to 1000\n",
    "    if len(content.split()) > 1000:\n",
    "        content = \" \".join(content.split()[:1000])\n",
    "\n",
    "    return content\n",
    "\n",
    "def preprocess_regular_file(content):\n",
    "    result = chardet.detect(content)\n",
    "    encoding = result[\"encoding\"]\n",
    "\n",
    "    if encoding is None:\n",
    "        encoding = \"utf-8\"\n",
    "\n",
    "    try:\n",
    "        decoded_content = content.decode(encoding, errors=\"ignore\")\n",
    "        if len(decoded_content.split()) > 500:\n",
    "            decoded_content = \" \".join(decoded_content.split()[:500])\n",
    "\n",
    "        return decoded_content\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Error: Failed to decode file content.\")\n",
    "\n",
    "def preprocess_code_cell(cell):\n",
    "    # Implement your preprocessing logic for code cells within Jupyter notebooks\n",
    "    # You can limit the token count or handle large code cells as necessary\n",
    "    # Example: Limit the token count to 200\n",
    "    if len(cell[\"source\"].split()) > 200:\n",
    "        cell[\"source\"] = \" \".join(cell[\"source\"].split()[:200])\n",
    "\n",
    "    return cell[\"source\"]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
